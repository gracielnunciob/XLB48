{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression MLP and SGD for Classifying Music Emotions\n",
    "---\n",
    "## About the Dataset\n",
    "\n",
    "For this experiment, our dataset contains 267 music samples that were labelled by 2 music experts. The dataset has a combination of high level and low level features. The high level features are tempo, arousal, and scale, which were all labelled by the music experts. The low level features were extracted from the music samples using the JAudio software.\n",
    "\n",
    "---\n",
    "## Libraries\n",
    "\n",
    "We used the pandas and csv libraries to load the dataset, which is a csv file. Then we used numpy to manipulate the loaded dataset. We used the sci kit learn library for the logistic regression, multilayer perceptron and stochastic gradient descent  algorithms. The XLB file is where we modularized the training and validation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "import numpy as np\n",
    "from scipy.stats import uniform, norm\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn import svm\n",
    "from XLB import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the data from the csv file and preprocess it by removing the header row and the filename column this is done by the extract_data function from the XLB file which takes the filename of the dataset as a parameter\n",
    "\n",
    "---\n",
    "\n",
    "After the initial preprocessing step, we sperated the music features and the labelled emotion into the x_train and y_train arrays respectively. We also used the scikit learn min max scaler to normalize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from files\n",
    "x_train, y_train = extract_data(\"FinalTrainingSet.csv\")\n",
    "x_val, y_val = extract_data(\"Validation Set.csv\")\n",
    "\n",
    "# scale data values\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "# feature selection\n",
    "num_features = 45\n",
    "feat_sel = VarianceThreshold()\n",
    "x_train = feat_sel.fit_transform(x_train)\n",
    "# feat_sel_2 = SelectFromModel(estimator=DecisionTreeClassifier(random_state=481516234))\n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=RandomForestClassifier(n_estimators=50,\\\n",
    "#                                              random_state=481516234))\n",
    "# 33.33% 0.24\n",
    "feat_sel_2 = SelectFromModel(\\\n",
    "                estimator=LogisticRegression(random_state=481516234))\n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=svm.LinearSVC(C=0.25, penalty=\"l1\", dual=False,\\\n",
    "#                                     random_state=481516234))\n",
    "# feat_sel_2 = SelectKBest(mutual_info_classif,k=num_features)\n",
    "x_train = feat_sel_2.fit_transform(x_train,y_train)\n",
    "x_val = feat_sel_2.transform(feat_sel.transform(x_val))\n",
    "# print(\"After Variance Threshold Feature Selection:\",x_train.shape)\n",
    "\n",
    "rand_seed = 3454132\n",
    "\n",
    "oversampler = SMOTE(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "x_smote, y_smote = oversampler.fit_resample(x_train,y_train)\n",
    "print(x_smote.shape,y_smote.shape)\n",
    "\n",
    "oversampler = RandomOverSampler(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "x_os, y_os = oversampler.fit_resample(x_train,y_train)\n",
    "print(x_os.shape,y_os.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = [\"Calm\",\"Cheerful\",\"Bravery\",\"Fearful\",\"Love\",\"Sadness\"]\n",
    "ovr_train = []\n",
    "ovr_val = []\n",
    "ovr_y_smote = []\n",
    "ovr_y_os = []\n",
    "for i in range(1,7):\n",
    "    ovr_train.append(ovr_labels(y_train, i))\n",
    "    ovr_val.append(ovr_labels(y_val,i))\n",
    "    ovr_y_os.append(ovr_labels(y_os,i))\n",
    "    ovr_y_smote.append(ovr_labels(y_smote,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform different methods to deal with imbalanced datasets. We used the oversampling and SMOTE methodology "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells are the experiments we ran with the default dataset, oversampled version, and SMOTE version using random search as the hyperparameter search algorithm\n",
    "\n",
    "---\n",
    "This cell instantiates the logistic regression object and the hyperparameter search object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression, RandomizedSearch\n",
    "random_search_iterations = 300\n",
    "k_folds = 10\n",
    "rand_seed = 3249807\n",
    "\n",
    "parameters = {\n",
    "    'penalty':['l1','l2', 'elasticnet', 'none'], \n",
    "    'dual' : [True, False],\n",
    "    'C': uniform(loc=0,scale=4),\n",
    "    'fit_intercept' : [True,False],\n",
    "    'class_weight' : ['balanced', None],\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'sag', 'liblinear', 'saga'],\n",
    "    'max_iter' : [100,200],\n",
    "    'l1_ratio' : uniform(loc=0,scale=1),\n",
    "    'warm_start': [True, False]\n",
    "}\n",
    "mnb = LogisticRegression(random_state=rand_seed)\n",
    "random_search_logreg = RandomizedSearchCV(mnb, parameters,cv=k_folds,\\\n",
    "                                          n_iter=random_search_iterations,\\\n",
    "                                          random_state=rand_seed,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells are where the logistice regression model is trained using the imbalanced, oversampled, and SMOTE. They are then validated using the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    if(i > 0):\n",
    "        print()\n",
    "    print_res(\"Vanilla\",x_train,ovr_train[i],random_search_logreg, themes[i], verbose = False)\n",
    "    test_res(\"Vanilla\",x_val,ovr_val[i],random_search_logreg, themes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    if(i > 0):\n",
    "        print()\n",
    "    print_res(\"Random Oversampling\",x_os,ovr_y_os[i],random_search_logreg,themes[i], verbose = False)\n",
    "    test_res(\"Random Oversampling\",x_val,ovr_val[i],random_search_logreg,themes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    if(i > 0):\n",
    "        print()\n",
    "    print_res(\"SMOTE\",x_smote,ovr_y_smote[i],random_search_logreg,themes[i])\n",
    "    test_res(\"SMOTE\",x_val,ovr_val[i],random_search_logreg,themes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell instantiates the multilayer perceptron object and the hyperparameter search object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from files\n",
    "x_train, y_train = extract_data(\"FinalTrainingSet.csv\")\n",
    "x_val, y_val = extract_data(\"Validation Set.csv\")\n",
    "\n",
    "# scale data values\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "# feature selection\n",
    "num_features = 45\n",
    "feat_sel = VarianceThreshold()\n",
    "x_train = feat_sel.fit_transform(x_train)\n",
    "# feat_sel_2 = SelectFromModel(estimator=DecisionTreeClassifier(random_state=481516234))\n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=RandomForestClassifier(n_estimators=50,\\\n",
    "#                                              random_state=481516234))\n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=LogisticRegression(random_state=481516234))\n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=svm.LinearSVC(C=0.25, penalty=\"l1\", dual=False,\\\n",
    "#                                     random_state=481516234))\n",
    "# f_classif 47.62% 0.43\n",
    "feat_sel_2 = SelectKBest(f_classif,k=num_features)\n",
    "x_train = feat_sel_2.fit_transform(x_train,y_train)\n",
    "x_val = feat_sel_2.transform(feat_sel.transform(x_val))\n",
    "# print(\"After Variance Threshold Feature Selection:\",x_train.shape)\n",
    "\n",
    "rand_seed = 3454132\n",
    "\n",
    "oversampler = SMOTE(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "x_smote, y_smote = oversampler.fit_resample(x_train,y_train)\n",
    "print(x_smote.shape,y_smote.shape)\n",
    "\n",
    "oversampler = RandomOverSampler(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "x_os, y_os = oversampler.fit_resample(x_train,y_train)\n",
    "print(x_os.shape,y_os.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPClassifier, RandomizedSearch\n",
    "random_search_iterations = 50\n",
    "k_folds = 10\n",
    "rand_seed = 3249807\n",
    "\n",
    "parameters = {\n",
    "    'activation':['logistic', 'tanh', 'relu'], \n",
    "    'solver' : ['lbfgs', 'sgd', 'adam'],\n",
    "    'alpha': [3e-4],\n",
    "    'batch_size' : [min(200,int(np.power(2,i))) for i in range(4,8)],\n",
    "    'learning_rate' : ['constant', 'invscaling', 'adaptive'],\n",
    "    'max_iter' : [700],\n",
    "    'shuffle' : [True,False],\n",
    "    'momentum' : uniform(loc=0.2,scale=0.8),\n",
    "    'nesterovs_momentum' : [True,False],\n",
    "    'early_stopping' : [True,False]\n",
    "}\n",
    "mlp = MLPClassifier(random_state=rand_seed)\n",
    "random_search_mlp = RandomizedSearchCV(mlp, parameters,cv=k_folds,\\\n",
    "                                       n_iter=random_search_iterations,\\\n",
    "                                       random_state=rand_seed, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells are where the multilayer perceptron model is trained using the imbalanced, oversampled, and SMOTE. They are then validated using the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    if(i > 0):\n",
    "        print()\n",
    "    print_res(\"Vanilla\",x_train,ovr_train[i],random_search_mlp, themes[i], verbose = False)\n",
    "    test_res(\"Vanilla\",x_val,ovr_val[i],random_search_mlp, themes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    if(i > 0):\n",
    "        print()\n",
    "    print_res(\"Random Oversampling\",x_os,ovr_y_os[i],random_search_mlp,themes[i], verbose = False)\n",
    "    test_res(\"Random Oversampling\",x_val,ovr_val[i],random_search_mlp,themes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    if(i > 0):\n",
    "        print()\n",
    "    print_res(\"SMOTE\",x_smote,ovr_y_smote[i],random_search_mlp,themes[i])\n",
    "    test_res(\"SMOTE\",x_val,ovr_val[i],random_search_mlp,themes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell instantiates the Stochastic Gradient Descent object and the hyperparameter search object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from files\n",
    "x_train, y_train = extract_data(\"FinalTrainingSet.csv\")\n",
    "x_val, y_val = extract_data(\"Validation Set.csv\")\n",
    "\n",
    "# scale data values\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "# feature selection\n",
    "num_features = 45\n",
    "feat_sel = VarianceThreshold()\n",
    "x_train = feat_sel.fit_transform(x_train)\n",
    "# feat_sel_2 = SelectFromModel(estimator=DecisionTreeClassifier(random_state=481516234))\n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=RandomForestClassifier(n_estimators=50,\\\n",
    "#                                              random_state=481516234))\n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=LogisticRegression(random_state=481516234))\n",
    "# 28.57% 0.28\n",
    "feat_sel_2 = SelectFromModel(\\\n",
    "                estimator=svm.LinearSVC(C=0.25, penalty=\"l1\", dual=False,\\\n",
    "                                    random_state=481516234))\n",
    "feat_sel_2 = SelectKBest(f_classif,k=num_features)\n",
    "x_train = feat_sel_2.fit_transform(x_train,y_train)\n",
    "x_val = feat_sel_2.transform(feat_sel.transform(x_val))\n",
    "# print(\"After Variance Threshold Feature Selection:\",x_train.shape)\n",
    "\n",
    "rand_seed = 3454132\n",
    "\n",
    "oversampler = SMOTE(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "x_smote, y_smote = oversampler.fit_resample(x_train,y_train)\n",
    "print(x_smote.shape,y_smote.shape)\n",
    "\n",
    "oversampler = RandomOverSampler(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "x_os, y_os = oversampler.fit_resample(x_train,y_train)\n",
    "print(x_os.shape,y_os.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGDClassifier, RandomizedSearch\n",
    "random_search_iterations = 200\n",
    "k_folds = 10\n",
    "rand_seed = 3249807\n",
    "\n",
    "parameters = {\n",
    "    'loss' : [ 'hinge', 'log', 'modified_huber', 'squared_hinge', \\\n",
    "              'perceptron', 'squared_loss', 'huber', \\\n",
    "              'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "    'penalty' : ['l2', 'l1', 'elasticnet'],\n",
    "    'alpha': [3e-4],\n",
    "    'l1_ratio': uniform(loc=0,scale=1),\n",
    "    'fit_intercept' : [True, False],\n",
    "    'max_iter' : [1000],\n",
    "    'shuffle' : [True, False],\n",
    "    'eta0' : uniform(loc=0.01,scale=0.99),\n",
    "    'learning_rate' : ['constant', 'invscaling', 'adaptive'],\n",
    "    'early_stopping' : [True, False],\n",
    "    'warm_start' : [True, False],\n",
    "    'average' : [True,False]\n",
    "}\n",
    "sgdc = SGDClassifier(random_state=rand_seed)\n",
    "random_search_sgdc = RandomizedSearchCV(sgdc, parameters,cv=k_folds,\\\n",
    "                                        n_iter=random_search_iterations,\\\n",
    "                                        random_state=rand_seed, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells are where the stochastic gradient descent model is trained using the imbalanced, oversampled, and SMOTE. They are then validated using the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    if(i > 0):\n",
    "        print()\n",
    "    print_res(\"Vanilla\",x_train,ovr_train[i],random_search_sgdc, themes[i], verbose = False)\n",
    "    test_res(\"Vanilla\",x_val,ovr_val[i],random_search_sgdc, themes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    if(i > 0):\n",
    "        print()\n",
    "    print_res(\"Random Oversampling\",x_os,ovr_y_os[i],random_search_sgdc,themes[i], verbose = False)\n",
    "    test_res(\"Random Oversampling\",x_val,ovr_val[i],random_search_sgdc,themes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    if(i > 0):\n",
    "        print()\n",
    "    print_res(\"SMOTE\",x_smote,ovr_y_smote[i],random_search_sgdc,themes[i])\n",
    "    test_res(\"SMOTE\",x_val,ovr_val[i],random_search_sgdc,themes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
