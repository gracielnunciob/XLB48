{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import uniform, norm\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2, f_classif, mutual_info_classif, VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.naive_bayes import CategoricalNB, ComplementNB, MultinomialNB, GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import svm\n",
    "from XLB import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all(filename_train,filename_val,selector,model_selector,name=\"\"):\n",
    "    # extract data from files\n",
    "    x_train, y_train = extract_data(filename_train)\n",
    "    x_val, y_val = extract_data(filename_val)\n",
    "\n",
    "    # scale data values\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_val = scaler.transform(x_val)\n",
    "\n",
    "    # feature selection\n",
    "    feat_sel = VarianceThreshold()\n",
    "    x_train = feat_sel.fit_transform(x_train)\n",
    "    x_train = selector.fit_transform(x_train,y_train)\n",
    "    x_val = selector.transform(feat_sel.transform(x_val))\n",
    "    \n",
    "    rand_seed = 3454132\n",
    "\n",
    "    oversampler = SMOTE(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "    x_smote, y_smote = oversampler.fit_resample(x_train,y_train)\n",
    "#     print(x_smote.shape,y_smote.shape)\n",
    "\n",
    "    oversampler = RandomOverSampler(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "    x_os, y_os = oversampler.fit_resample(x_train,y_train)\n",
    "#     print(x_os.shape,y_os.shape)\n",
    "    print(\"{} Results:\".format(name))\n",
    "    print_res(\"Vanilla\",x_train,y_train,model_selector)\n",
    "    test_res(\"Vanilla\",x_val,y_val,model_selector)\n",
    "    print_res(\"Random Oversampling\",x_os,y_os,model_selector)\n",
    "    test_res(\"Random Oversampling\",x_val,y_val,model_selector)\n",
    "    print_res(\"SMOTE\",x_smote,y_smote,model_selector)\n",
    "    test_res(\"SMOTE\",x_val,y_val,model_selector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 45\n",
    "selectors = {\n",
    "    \"chi2\" : SelectKBest(chi2,k=num_features),\n",
    "    \"f_classif\" : SelectKBest(f_classif,k=num_features),\n",
    "    \"mutual_info_classif\" : SelectKBest(mutual_info_classif,k=num_features),\n",
    "    \"FromModel DT\" : SelectFromModel(estimator=tree.DecisionTreeClassifier(random_state=481516234)),\n",
    "    \"RandForest\" : SelectFromModel(estimator=RandomForestClassifier(n_estimators=100,random_state=481516234)),\n",
    "    \"LogReg\" : SelectFromModel(estimator=LogisticRegression(random_state=481516234)),\n",
    "    \"LinearSVC\" : SelectFromModel(estimator=svm.LinearSVC(C=0.25, penalty=\"l1\", dual=False,random_state=481516234)),\n",
    "}\n",
    "# ComplementNB, RandomizedSearch\n",
    "random_search_iterations = 1000\n",
    "k_folds = 10\n",
    "rand_seed = 3249807\n",
    "\n",
    "parameters = {\n",
    "    'alpha':uniform(loc=0,scale=1.0), \n",
    "    'fit_prior':[True, False], \n",
    "    'norm' : [True, False]\n",
    "}\n",
    "cnb = ComplementNB()\n",
    "random_search_complement = RandomizedSearchCV(cnb, parameters,cv=k_folds,\\\n",
    "                                              n_iter=random_search_iterations,\\\n",
    "                                              random_state=rand_seed,n_jobs=-1)\n",
    "for k,v in selectors.items():\n",
    "    train_all(\"FinalTrainingSet.csv\",\"Validation Set.csv\",v,random_search_complement,k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 45\n",
    "selectors = {\n",
    "    \"chi2\" : SelectKBest(chi2,k=num_features),\n",
    "    \"f_classif\" : SelectKBest(f_classif,k=num_features),\n",
    "    \"mutual_info_classif\" : SelectKBest(mutual_info_classif,k=num_features),\n",
    "    \"FromModel DT\" : SelectFromModel(estimator=tree.DecisionTreeClassifier(random_state=481516234)),\n",
    "    \"RandForest\" : SelectFromModel(estimator=RandomForestClassifier(n_estimators=100,random_state=481516234)),\n",
    "    \"LogReg\" : SelectFromModel(estimator=LogisticRegression(random_state=481516234)),\n",
    "    \"LinearSVC\" : SelectFromModel(estimator=svm.LinearSVC(C=0.25, penalty=\"l1\", dual=False,random_state=481516234)),\n",
    "}\n",
    "# CategoricalNB, RandomizedSearch\n",
    "random_search_iterations = 1000\n",
    "k_folds = 10\n",
    "\n",
    "parameters = {\n",
    "    'alpha':uniform(loc=0,scale=1.0), \n",
    "    'fit_prior':[True, False]\n",
    "}\n",
    "canb = CategoricalNB()\n",
    "random_search_categorical = RandomizedSearchCV(canb, parameters,cv=k_folds,\\\n",
    "                                           n_iter=random_search_iterations,\\\n",
    "                                            random_state=rand_seed)\n",
    "for k,v in selectors.items():\n",
    "    train_all(\"FinalTrainingSet.csv\",\"Validation Set.csv\",v,random_search_categorical,k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 45\n",
    "selectors = {\n",
    "    \"chi2\" : SelectKBest(chi2,k=num_features),\n",
    "    \"f_classif\" : SelectKBest(f_classif,k=num_features),\n",
    "    \"mutual_info_classif\" : SelectKBest(mutual_info_classif,k=num_features),\n",
    "    \"FromModel DT\" : SelectFromModel(estimator=tree.DecisionTreeClassifier(random_state=481516234)),\n",
    "    \"RandForest\" : SelectFromModel(estimator=RandomForestClassifier(n_estimators=100,random_state=481516234)),\n",
    "    \"LogReg\" : SelectFromModel(estimator=LogisticRegression(random_state=481516234)),\n",
    "    \"LinearSVC\" : SelectFromModel(estimator=svm.LinearSVC(C=0.25, penalty=\"l1\", dual=False,random_state=481516234)),\n",
    "}\n",
    "# MultinomialNB, RandomizedSearch\n",
    "random_search_iterations = 1000\n",
    "k_folds = 10\n",
    "\n",
    "parameters = {\n",
    "    'alpha':uniform(loc=0,scale=1.0), \n",
    "    'fit_prior':[True, False]\n",
    "}\n",
    "mnb = MultinomialNB()\n",
    "random_search_multinomial = RandomizedSearchCV(mnb, parameters,cv=k_folds,\\\n",
    "                                            n_iter=random_search_iterations,\\\n",
    "                                            random_state=rand_seed,n_jobs=-1)\n",
    "for k,v in selectors.items():\n",
    "    train_all(\"FinalTrainingSet.csv\",\"Validation Set.csv\",v,random_search_multinomial,k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
