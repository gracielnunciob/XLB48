{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier for Classifying Music Emotions\n",
    "---\n",
    "## About the Dataset\n",
    "\n",
    "For this experiment, our dataset contains 267 music samples that were labelled by 2 music experts and 151 samples labeled in the semi-supervised learning task. The dataset has a combination of high level and low level features. The high level features are tempo, arousal, and scale, which were all labelled by the music experts. The low level features were extracted from the music samples using the JAudio software.\n",
    "\n",
    "---\n",
    "## Libraries\n",
    "\n",
    "We used the pandas and csv libraries to load the dataset, which is a csv file. Then we used numpy to manipulate the loaded dataset. We used the sci kit learn library for the decision tree algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import uniform, norm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, VarianceThreshold, chi2, f_classif, mutual_info_classif\n",
    "from XLB import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the data from the csv file and preprocess it by removing the header row and the filename column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the initial preprocessing step, we separated the music features and the labelled emotion into the x_train and y_train arrays respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 45) (750,)\n",
      "(750, 45) (750,)\n"
     ]
    }
   ],
   "source": [
    "# extract data from files\n",
    "x_train, y_train = extract_data(\"FinalTrainingSet.csv\")\n",
    "x_val, y_val = extract_data(\"Validation Set.csv\")\n",
    "\n",
    "# scale data values\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "# feature selection\n",
    "num_features = 45\n",
    "feat_sel = VarianceThreshold()\n",
    "x_train = feat_sel.fit_transform(x_train)\n",
    "# feat_sel_2 = SelectFromModel(estimator=DecisionTreeClassifier(random_state=481516234))\n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=RandomForestClassifier(n_estimators=50,\\\n",
    "#                                              random_state=481516234))\n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=LogisticRegression(random_state=481516234))\n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=svm.LinearSVC(C=0.25, penalty=\"l1\", dual=False,\\\n",
    "#                                     random_state=481516234))\n",
    "# WINNER: 47.62% 0.45\n",
    "feat_sel_2 = SelectKBest(mutual_info_classif,k=num_features)\n",
    "x_train = feat_sel_2.fit_transform(x_train,y_train)\n",
    "x_val = feat_sel_2.transform(feat_sel.transform(x_val))\n",
    "# print(\"After Variance Threshold Feature Selection:\",x_train.shape)\n",
    "\n",
    "rand_seed = 3454132\n",
    "\n",
    "oversampler = SMOTE(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "x_smote, y_smote = oversampler.fit_resample(x_train,y_train)\n",
    "print(x_smote.shape,y_smote.shape)\n",
    "\n",
    "oversampler = RandomOverSampler(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "x_os, y_os = oversampler.fit_resample(x_train,y_train)\n",
    "print(x_os.shape,y_os.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = [\"Calm\",\"Cheerful\",\"Bravery\",\"Fearful\",\"Love\",\"Sadness\"]\n",
    "ovr_train = []\n",
    "ovr_val = []\n",
    "ovr_y_smote = []\n",
    "ovr_y_os = []\n",
    "for i in range(1,7):\n",
    "    ovr_train.append(ovr_labels(y_train, i))\n",
    "    ovr_val.append(ovr_labels(y_val,i))\n",
    "    ovr_y_os.append(ovr_labels(y_os,i))\n",
    "    ovr_y_smote.append(ovr_labels(y_smote,i))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform different methods to deal with imbalanced datasets. We used the oversampling *Insert link to oversampling if needed* and SMOTE methodology *Insert link to SMOTE if needed*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we defined a function that will train the model and perform the hyperparameter search. x_train is the input data, y_train is the labels for the input data, and hp_search is the algorithm for hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function trains the model, using the previously defined train_model function, and displays the confusion matrix of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells are the experiments we ran with the default dataset, oversampled version, and SMOTE version using random search as the hyperparameter search algorithm\n",
    "\n",
    "---\n",
    "This cell instantiates the decison tree object and the hyperparameter search object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "random_search_iterations = 1000\n",
    "n_splits = 5\n",
    "rand_seed = 481516234\n",
    "\n",
    "parameters = {\n",
    "    'criterion':[\"gini\", \"entropy\"], \n",
    "    'splitter':[\"best\", \"random\"], \n",
    "    'max_depth':range(1, num_features + 1), \n",
    "    'min_samples_split': uniform(loc=0.00,scale=1.0),\n",
    "    'min_samples_leaf': uniform(loc=0.0001,scale=0.4999)\n",
    "}\n",
    "\n",
    "# train classifier for SMOTE data\n",
    "dt = tree.DecisionTreeClassifier(random_state=rand_seed)\n",
    "rscv = RandomizedSearchCV(dt, parameters,cv=n_splits, random_state=rand_seed, n_iter=random_search_iterations,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is trained using the default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    if(i > 0):\n",
    "        print()\n",
    "    print_res(\"Vanilla\",x_train,ovr_train[i],rscv, themes[i], verbose = False)\n",
    "    disp_tree(rscv.best_estimator_,\"FinalTrainingSet.csv\")\n",
    "    test_res(\"Vanilla\",x_val,ovr_val[i],rscv, themes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    if(i > 0):\n",
    "        print()\n",
    "    print_res(\"Random Oversampling\",x_os,ovr_y_os[i],rscv,themes[i], verbose = False)\n",
    "    disp_tree(rscv.best_estimator_,\"FinalTrainingSet.csv\")\n",
    "    test_res(\"Random Oversampling\",x_val,ovr_val[i],rscv,themes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    if(i > 0):\n",
    "        print()\n",
    "    print_res(\"SMOTE\",x_smote,ovr_y_smote[i],rscv,themes[i])\n",
    "    disp_tree(rscv.best_estimator_,\"FinalTrainingSet.csv\")\n",
    "    test_res(\"SMOTE\",x_val,ovr_val[i],rscv,themes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from files\n",
    "x_train, y_train = extract_data(\"FinalTrainingSet.csv\")\n",
    "x_val, y_val = extract_data(\"Validation Set.csv\")\n",
    "\n",
    "# scale data values\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "# feature selection\n",
    "num_features = 45\n",
    "feat_sel = VarianceThreshold()\n",
    "x_train = feat_sel.fit_transform(x_train)\n",
    "# feat_sel_2 = SelectFromModel(estimator=DecisionTreeClassifier(random_state=481516234))\n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=RandomForestClassifier(n_estimators=100,\\\n",
    "#                                              random_state=481516234))\n",
    "# 38.10% 0.36\n",
    "feat_sel_2 = SelectFromModel(\\\n",
    "                estimator=LogisticRegression(random_state=481516234))\n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=svm.LinearSVC(C=0.25, penalty=\"l1\", dual=False,\\\n",
    "#                                     random_state=481516234))\n",
    "# feat_sel_2 = SelectKBest(mutual_info_classif,k=num_features)\n",
    "x_train = feat_sel_2.fit_transform(x_train,y_train)\n",
    "x_val = feat_sel_2.transform(feat_sel.transform(x_val))\n",
    "# print(\"After Variance Threshold Feature Selection:\",x_train.shape)\n",
    "\n",
    "rand_seed = 3454132\n",
    "\n",
    "oversampler = SMOTE(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "x_smote, y_smote = oversampler.fit_resample(x_train,y_train)\n",
    "print(x_smote.shape,y_smote.shape)\n",
    "\n",
    "oversampler = RandomOverSampler(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "x_os, y_os = oversampler.fit_resample(x_train,y_train)\n",
    "print(x_os.shape,y_os.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "random_search_iterations = 1000\n",
    "n_splits = 5\n",
    "rand_seed = 108750183\n",
    "\n",
    "parameters = {\n",
    "    'n_estimators' : [i for i in range(90,151)],\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'max_depth' : [i for i in range(1,num_features + 1)],\n",
    "    'min_samples_split' : uniform(loc=0.01,scale=0.99),\n",
    "    'min_samples_leaf' : uniform(loc=0.01,scale=0.49),\n",
    "    'bootstrap' : [True, False],\n",
    "    'warm_start' : [True, False],\n",
    "}\n",
    "\n",
    "# train classifier for SMOTE data\n",
    "random_forest = RandomForestClassifier(random_state=rand_seed)\n",
    "random_search_random_forest = \\\n",
    "                        RandomizedSearchCV(random_forest, parameters,\\\n",
    "                                           cv=n_splits, \\\n",
    "                                           random_state=rand_seed, \\\n",
    "                                           n_iter=random_search_iterations,\\\n",
    "                                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    if(i > 0):\n",
    "        print()\n",
    "    print_res(\"Vanilla\",x_train,ovr_train[i],random_search_random_forest, themes[i], verbose = False)\n",
    "    test_res(\"Vanilla\",x_val,ovr_val[i],random_search_random_forest, themes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    if(i > 0):\n",
    "        print()\n",
    "    print_res(\"Random Oversampling\",x_os,ovr_y_os[i],random_search_random_forest,themes[i], verbose = False)\n",
    "    test_res(\"Random Oversampling\",x_val,ovr_val[i],random_search_random_forest,themes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    if(i > 0):\n",
    "        print()\n",
    "    print_res(\"SMOTE\",x_smote,ovr_y_smote[i],random_search_random_forest,themes[i])\n",
    "    test_res(\"SMOTE\",x_val,ovr_val[i],random_search_random_forest,themes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
