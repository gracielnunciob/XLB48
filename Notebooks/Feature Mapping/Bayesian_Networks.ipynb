{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "import numpy as np\n",
    "from scipy.stats import uniform, norm\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn import svm\n",
    "from XLB import *\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchbnn as bnn\n",
    "from sklearn import datasets\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 69) (750,)\n",
      "(750, 69) (750,)\n",
      "[1.         1.         1.         0.52108434 0.48107515 0.72424866\n",
      " 0.43380062 0.52308697 0.52817024 0.12486843 0.54195949 0.3611159\n",
      " 0.52129297 0.27518074 0.46655604 0.6557498  0.30235053 0.32643465\n",
      " 0.02736892 0.55518878 0.59222226 0.09831908 0.54163232 0.36127085\n",
      " 0.52209661 0.27414381 0.46502436 0.6529655  0.30123643 0.72383199\n",
      " 0.65381368 0.4523244  0.13277089 0.18917784 0.09148758 0.50564972\n",
      " 0.77948653 0.68639304 0.09099316 0.01541819 0.71095813 0.7826087\n",
      " 0.08241345 0.77948653 0.6864991  0.0910664  0.0145165  0.70895515\n",
      " 0.77009661 0.67647059 0.00203913 0.00562726 0.01349145 0.01986507\n",
      " 0.0123881  0.01746491 0.0123881  0.01746491 0.00204347 0.00454104\n",
      " 0.00204347 0.00454104 0.00284424 0.25971479 0.3756087  0.20311342\n",
      " 0.23618019 0.17934551 0.1793323 ]\n"
     ]
    }
   ],
   "source": [
    "# extract data from files\n",
    "x_train, y_train = extract_data(\"FinalTrainingSet.csv\")\n",
    "x_val, y_val = extract_data(\"Validation Set.csv\")\n",
    "\n",
    "#scale data values\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "# feature selection\n",
    "num_features = 45\n",
    "feat_sel = VarianceThreshold()\n",
    "x_train = feat_sel.fit_transform(x_train)\n",
    "# feat_sel_2 = SelectFromModel(estimator=DecisionTreeClassifier(random_state=481516234))\n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=RandomForestClassifier(n_estimators=50,\\\n",
    "#                                              random_state=481516234))\n",
    "# 33.33% 0.24\n",
    "feat_sel_2 = SelectFromModel(\\\n",
    "                estimator=LogisticRegression(random_state=481516234))\n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=svm.LinearSVC(C=0.25, penalty=\"l1\", dual=False,\\\n",
    "#                                     random_state=481516234))\n",
    "# feat_sel_2 = SelectKBest(mutual_info_classif,k=num_features)\n",
    "x_train = feat_sel_2.fit_transform(x_train,y_train)\n",
    "x_val = feat_sel_2.transform(feat_sel.transform(x_val))\n",
    "# print(\"After Variance Threshold Feature Selection:\",x_train.shape)\n",
    "\n",
    "rand_seed = 3454132\n",
    "\n",
    "oversampler = SMOTE(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "x_smote, y_smote = oversampler.fit_resample(x_train,y_train)\n",
    "print(x_smote.shape,y_smote.shape)\n",
    "\n",
    "oversampler = RandomOverSampler(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "x_os, y_os = oversampler.fit_resample(x_train,y_train)\n",
    "print(x_os.shape,y_os.shape)\n",
    "\n",
    "pd.cut(x_train[1], bins=3, labels=np.arange(3), right=False)\n",
    "\n",
    "print(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = [\"Calm\",\"Cheerful\",\"Bravery\",\"Fearful\",\"Love\",\"Sadness\"]\n",
    "ovr_train = []\n",
    "ovr_val = []\n",
    "ovr_y_smote = []\n",
    "ovr_y_os = []\n",
    "for i in range(1,7):\n",
    "    ovr_train.append(ovr_labels(y_train, i))\n",
    "    ovr_val.append(ovr_labels(y_val,i))\n",
    "    ovr_y_os.append(ovr_labels(y_os,i))\n",
    "    ovr_y_smote.append(ovr_labels(y_smote,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractCustomModel:\n",
    "#     \"\"\"\n",
    "#     This is the default constructor for this class.\n",
    " \n",
    "#     Parameters:\n",
    "#     params : dict - hyperparameters for the model\n",
    "#     \"\"\"\n",
    "    def __init__(self,params):\n",
    "        self.params = params\n",
    "\n",
    "#     This method trains the model on the given dataset\n",
    " \n",
    "#     Parameters:\n",
    "#     x_train : numpy.ndarray - training set data\n",
    "#     \"\"\"\n",
    "    def train(self,x_train,y_train=None):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "#     This method evaluates the performance of the model on the given test set.\n",
    " \n",
    "#     Parameters:\n",
    "#     x_test : numpy.ndarray - test set data\n",
    " \n",
    "#     Returns:\n",
    "#     A floating point number which measures the performance of the model on the \n",
    "#     test set.\n",
    "#     \"\"\"\n",
    "\n",
    "    def evaluate(self,x_test,y_test=None):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AbstractCustomModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-30369e97cfe6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# This class is an adapter for the Bayesian Network library.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mBayesianNetworkAdapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAbstractCustomModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#  This is the default constructor for this class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AbstractCustomModel' is not defined"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# This class is an adapter for the Bayesian Network library.\n",
    "# \"\"\"\n",
    "class BayesianNetworkAdapter(AbstractCustomModel):\n",
    "    \n",
    "#  This is the default constructor for this class.\n",
    " \n",
    "#     Parameters:\n",
    "#     params : dict - hyperparameters for the model\n",
    "#     \"\"\"\n",
    "    def __init__(self,params):\n",
    "        self.params = params\n",
    "        super().__init__()\n",
    " \n",
    " \n",
    "#     Parameters:\n",
    "#     x_train : numpy.ndarray - training set data\n",
    "\n",
    "    def train(self,x_train,y_train=None):\n",
    "        #DATA IS X_TRAIN\n",
    "        data = x_train\n",
    "        # target = np.reshape(np.array(list(map(int,y_train))), (x_train.shape[0], 1))\n",
    "        target = y_train-1\n",
    "        # print(target.size)\n",
    "        print(y_train-1)\n",
    "        data_tensor= torch.from_numpy(data).float()\n",
    "        target_tensor= torch.from_numpy(target).long()\n",
    "\n",
    "        #Building the model\n",
    "        # prior_mu (Float) is the mean of prior normal distribution.\n",
    "        # prior_sigma (Float) is the sigma of prior normal distribution.\n",
    "        model = nn.Sequential(\n",
    "            bnn.BayesLinear(prior_mu=0.5, prior_sigma=0.15, in_features=69, out_features=100),\n",
    "            nn.ReLU(),\n",
    "            bnn.BayesLinear(prior_mu=0.5, prior_sigma=0.15, in_features=100, out_features=6),\n",
    "        )\n",
    "\n",
    "        #The two-loss functions used here are cross-entropy loss and the BKL loss\n",
    "        #which is used to compute the KL (Kullbackâ€“Leibler) divergence of the network.\n",
    "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        klloss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "        klweight = 0.01\n",
    "        optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "        #TRAINING\n",
    "        for step in range(500):\n",
    "            models = model(data_tensor)\n",
    "        #     print(models.size(0))\n",
    "            cross_entropy = cross_entropy_loss(models, target_tensor)\n",
    "            kl = klloss(model)\n",
    "            total_cost = cross_entropy + klweight*kl\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(models.data, 1)\n",
    "        final = target_tensor.size(0)\n",
    "        correct = (predicted == target_tensor).sum()\n",
    "        print('- Accuracy: %f %%' % (100 * float(correct) / final))\n",
    "        print('- CE : %2.2f, KL : %2.2f' % (cross_entropy.item(), kl.item()))\n",
    "        \n",
    "        pass\n",
    " \n",
    "   #   This method evaluates the performance of the model on the given test set.\n",
    "#     Parameters:\n",
    "#     x_test : numpy.ndarray - test set data\n",
    " \n",
    "#     Returns:\n",
    "#     A floating point number which measures the performance of the model on the \n",
    "#     test set.\n",
    "#     \"\"\"\n",
    "    def evaluate(self,x_test,y_test=None):\n",
    "        # TODO\n",
    "        pass\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 5. 5. 5. 5. 5. 5. 5.\n",
      " 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 3. 0. 3. 0. 0. 0. 0. 2. 3. 5. 0. 2. 2. 5.\n",
      " 0. 0. 0. 2. 0. 3. 0. 0. 2. 2. 2. 0. 2. 2. 2. 0. 0. 0. 3. 2. 1. 0. 1. 0.\n",
      " 0. 0. 3. 3. 0. 0. 0. 0. 0. 0. 1. 3. 3. 2. 2. 4. 0. 0. 1. 1. 0. 0. 5. 0.\n",
      " 3. 0. 1. 0. 3. 0. 5. 0. 0. 2. 0. 0. 3. 2. 2. 3. 5. 5. 3. 3. 0. 1. 0. 0.\n",
      " 1. 0. 0. 1. 3. 3. 4. 3. 2. 1. 3. 5. 5. 4. 3. 3. 4. 0. 3. 0. 2. 4. 0. 5.\n",
      " 3. 5. 5. 4. 4. 5. 4. 2. 1. 4. 1. 3. 1. 4. 0. 3. 5. 5. 5. 3. 0. 1. 5. 0.\n",
      " 4. 5. 1. 4. 4. 5. 1. 3. 5. 4. 5. 4. 5. 5. 3. 3. 4.]\n",
      "- Accuracy: 32.418953 %\n",
      "- CE : 1.69, KL : 5.92\n"
     ]
    }
   ],
   "source": [
    "# dataset = datasets.load_iris()\n",
    "\n",
    "#DATA IS X_TRAIN\n",
    "data = x_train\n",
    "# target = np.reshape(np.array(list(map(int,y_train))), (x_train.shape[0], 1))\n",
    "target = y_train-1\n",
    "# print(target.size)\n",
    "print(y_train-1)\n",
    "data_tensor= torch.from_numpy(data).float()\n",
    "target_tensor= torch.from_numpy(target).long()\n",
    "\n",
    "#Building the model\n",
    "# prior_mu (Float) is the mean of prior normal distribution.\n",
    "# prior_sigma (Float) is the sigma of prior normal distribution.\n",
    "model = nn.Sequential(\n",
    "    bnn.BayesLinear(prior_mu=0.5, prior_sigma=0.15, in_features=69, out_features=100),\n",
    "    nn.ReLU(),\n",
    "    bnn.BayesLinear(prior_mu=0.5, prior_sigma=0.15, in_features=100, out_features=6),\n",
    ")\n",
    "\n",
    "#The two-loss functions used here are cross-entropy loss and the BKL loss\n",
    "#which is used to compute the KL (Kullbackâ€“Leibler) divergence of the network.\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "klloss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "klweight = 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "#TRAINING\n",
    "for step in range(500):\n",
    "    models = model(data_tensor)\n",
    "#     print(models.size(0))\n",
    "    cross_entropy = cross_entropy_loss(models, target_tensor)\n",
    "    kl = klloss(model)\n",
    "    total_cost = cross_entropy + klweight*kl\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_cost.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "_, predicted = torch.max(models.data, 1)\n",
    "final = target_tensor.size(0)\n",
    "correct = (predicted == target_tensor).sum()\n",
    "print('- Accuracy: %f %%' % (100 * float(correct) / final))\n",
    "print('- CE : %2.2f, KL : %2.2f' % (cross_entropy.item(), kl.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-31e26152f523>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_iris\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "dataset = datasets.load_iris()\n",
    "print(dataset.target.size(0))\n",
    "\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
