{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from csv import reader\n",
    "from csv import writer\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import uniform, norm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, VarianceThreshold, chi2, f_classif, mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from XLB import *\n",
    "from apyori import apriori\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True False False False False False False False\n",
      "  True False False False False False  True False False False  True False\n",
      " False False  True False False False False False  True False False False\n",
      "  True False False False False False  True False False False  True False\n",
      " False False  True False False False False False False  True False  True\n",
      "  True  True  True False False False  True  True  True False  True  True\n",
      " False False  True  True  True False  True False False  True  True False\n",
      " False  True  True False False False  True  True  True False  True  True\n",
      " False False  True  True  True False  True False False False False False\n",
      " False False  True False  True False  True False  True False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False  True False  True  True\n",
      " False]\n",
      "(750, 69) (750,)\n",
      "(750, 69) (750,)\n"
     ]
    }
   ],
   "source": [
    "# extract data from files\n",
    "x_train, y_train = extract_data(\"FinalTrainingSet.csv\")\n",
    "x_val, y_val = extract_data(\"Validation Set.csv\")\n",
    "\n",
    "# scale data values\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "# feature selection\n",
    "num_features = 69\n",
    "feat_sel = VarianceThreshold()\n",
    "x_train = feat_sel.fit_transform(x_train)\n",
    "# 28.57% 0.25\n",
    "# feat_sel_2 = SelectFromModel(estimator=DecisionTreeClassifier(random_state=481516234))\n",
    "# 38.10% 0.29\n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=RandomForestClassifier(n_estimators=100,\\\n",
    "#                                              random_state=481516234))\n",
    "# 42.86% 0.36\n",
    "#feat_sel_2 = SelectFromModel(\\\n",
    "                #estimator=LogisticRegression(random_state=481516234))\n",
    "# \n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=svm.LinearSVC(C=0.25, penalty=\"l1\", dual=False,\\\n",
    "#                                     random_state=481516234))\n",
    "# \n",
    "feat_sel_2 = SelectKBest(chi2,k=num_features)\n",
    "x_train = feat_sel_2.fit_transform(x_train,y_train)\n",
    "print(feat_sel_2.get_support())\n",
    "x_val = feat_sel_2.transform(feat_sel.transform(x_val))\n",
    "# print(\"After Variance Threshold Feature Selection:\",x_train.shape)\n",
    "\n",
    "rand_seed = 3454132\n",
    "\n",
    "oversampler = SMOTE(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "x_smote, y_smote = oversampler.fit_resample(x_train,y_train)\n",
    "print(x_smote.shape,y_smote.shape)\n",
    "\n",
    "oversampler = RandomOverSampler(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "x_os, y_os = oversampler.fit_resample(x_train,y_train)\n",
    "print(x_os.shape,y_os.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(401, 70)\n",
      "['Tempo', 'Arousal', 'IsMajor', 'IsMinor', 'IsDissonant', 'Derivative of Root Mean Square Overall Standard Deviation', 'Derivative of Running Mean of Root Mean Square Overall Standard Deviation', 'Derivative of Running Mean of Spectral Flux Overall Standard Deviation', 'Derivative of Running Mean of Spectral Variability Overall Standard Deviation', 'Derivative of Spectral Flux Overall Standard Deviation', 'Derivative of Spectral Variability Overall Standard Deviation', 'Derivative of Standard Deviation of Root Mean Square Overall Standard Deviation', 'Derivative of Standard Deviation of Spectral Flux Overall Standard Deviation', 'Derivative of Standard Deviation of Spectral Variability Overall Standard Deviation', 'Fraction Of Low Energy Windows Overall Average', 'Magnitude Spectrum Overall Average', 'Magnitude Spectrum Overall Standard Deviation', 'Power Spectrum Overall Average', 'Power Spectrum Overall Standard Deviation', 'Running Mean of Compactness Overall Standard Deviation', 'Running Mean of Fraction Of Low Energy Windows Overall Average', 'Running Mean of Fraction Of Low Energy Windows Overall Standard Deviation', 'Running Mean of Root Mean Square Overall Standard Deviation', 'Running Mean of Spectral Centroid Overall Average', 'Running Mean of Spectral Flux Overall Standard Deviation', 'Running Mean of Spectral Rolloff Point Overall Average', 'Running Mean of Spectral Rolloff Point Overall Standard Deviation', 'Running Mean of Spectral Variability Overall Standard Deviation', 'Spectral Centroid Overall Average', 'Spectral Centroid Overall Standard Deviation', 'Spectral Rolloff Point Overall Average', 'Spectral Rolloff Point Overall Standard Deviation', 'Standard Deviation of Compactness Overall Standard Deviation', 'Standard Deviation of Fraction Of Low Energy Windows Overall Average', 'Standard Deviation of Fraction Of Low Energy Windows Overall Standard Deviation', 'Standard Deviation of Root Mean Square Overall Standard Deviation', 'Standard Deviation of Spectral Centroid Overall Average', 'Standard Deviation of Spectral Flux Overall Standard Deviation', 'Standard Deviation of Spectral Rolloff Point Overall Average', 'Standard Deviation of Spectral Rolloff Point Overall Standard Deviation', 'Standard Deviation of Spectral Variability Overall Standard Deviation', 'Beat Histogram Overall Standard Deviation', 'Beat Sum Overall Standard Deviation', 'ConstantQ Overall Standard Deviation', 'Derivative of Beat Sum Overall Standard Deviation', 'Derivative of LPC Overall Standard Deviation', 'Derivative of Method of Moments Overall Average', 'Derivative of Method of Moments Overall Standard Deviation', 'Derivative of MFCC Overall Average', 'Derivative of MFCC Overall Standard Deviation', 'Derivative of Partial Based Spectral Centroid Overall Average', 'Derivative of Partial Based Spectral Centroid Overall Standard Deviation', 'Derivative of Partial Based Spectral Flux Overall Average', 'Derivative of Partial Based Spectral Flux Overall Standard Deviation', 'Derivative of Peak Based Spectral Smoothness Overall Average', 'Derivative of Peak Based Spectral Smoothness Overall Standard Deviation', 'Derivative of Relative Difference Function Overall Average', 'Derivative of Relative Difference Function Overall Standard Deviation', 'Derivative of Running Mean of Beat Sum Overall Average', 'Derivative of Running Mean of Beat Sum Overall Standard Deviation', 'Derivative of Running Mean of LPC Overall Average', 'Derivative of Running Mean of LPC Overall Standard Deviation', 'Derivative of Running Mean of Method of Moments Overall Average', 'Derivative of Running Mean of Method of Moments Overall Standard Deviation', 'Derivative of Running Mean of MFCC Overall Average', 'Derivative of Running Mean of MFCC Overall Standard Deviation', 'Derivative of Running Mean of Partial Based Spectral Centroid Overall Standard Deviation', 'Derivative of Running Mean of Partial Based Spectral Flux Overall Standard Deviation', 'Derivative of Running Mean of Peak Based Spectral Smoothness Overall Average', 'Theme_numbered']\n",
      "(401, 70)\n"
     ]
    }
   ],
   "source": [
    "#MOVING FEATURE HEADERS INTO A LIST\n",
    "import csv\n",
    "\n",
    "\n",
    "# IsCalm = []\n",
    "# IsCheerful = []\n",
    "# IsBravery = []\n",
    "# IsFearful = []\n",
    "# IsLove = []\n",
    "# IsSad = []\n",
    "# Theme_Number = []  \n",
    "\n",
    "\n",
    "f = open(\"FinalTrainingSet.csv\")\n",
    "reader = csv.reader(f)\n",
    "features = next(reader)\n",
    "row = list(reader)\n",
    "\n",
    "csv_temp = pd.read_csv(\"FinalTrainingSet.csv\")\n",
    "Theme_numbered = csv_temp['Theme(Numbered)'].tolist()\n",
    "Theme_numbered = np.asarray(Theme_numbered) \n",
    "\n",
    "# print(Theme_numbered.shape)\n",
    "x_train = np.append(x_train, Theme_numbered.reshape(Theme_numbered.shape[0], 1), axis=1)\n",
    "print(x_train.shape)\n",
    "\n",
    "# x_train = np.delete(x_train, 69, axis=1)\n",
    "\n",
    "#Deleting everything except features from the dataset\n",
    "features.remove(\"Row Labels\")\n",
    "features.remove(\"Theme\")\n",
    "features.remove(\"Theme(Numbered)\")\n",
    "# print(len(features))\n",
    "#Retained features after selection\n",
    "selected_feats = feat_sel_2.get_support(True)\n",
    "\n",
    "for ind, ft in sorted(enumerate(features), reverse=True): \n",
    "    if ind not in selected_feats:\n",
    "        del features[ind]  \n",
    "        \n",
    "# np.append(x_train, Theme_numbered)\n",
    "features.append('Theme_numbered')\n",
    "\n",
    "new_column = pd.DataFrame({'Theme_numbered': Theme_numbered}) \n",
    "csv_temp = csv_temp.merge(new_column, left_index = True, right_index = True)\n",
    "\n",
    "column = csv_temp.Theme_numbered\n",
    "\n",
    "# column1 = csv_temp.IsCheerful\n",
    "# column2 = csv_temp.IsBravery\n",
    "# column3 = csv_temp.IsFearful\n",
    "# column4 = csv_temp.IsLove\n",
    "# column5 = csv_temp.IsSad \n",
    "\n",
    "\n",
    "# print(column)\n",
    "# print(column1)\n",
    "# print(column2)\n",
    "# print(column3)\n",
    "# print(column4)\n",
    "# print(column5)\n",
    "\n",
    "# csv_temp.to_csv('asd.csv', index = False\n",
    "\n",
    "print(features)\n",
    "row_count = len(row)\n",
    "f.close()\n",
    " \n",
    "print(x_train.shape)    \n",
    "#Printing modified list \n",
    "# print (features) \n",
    "# print (len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This abstract class represents a custom machine learning model.\n",
    "\"\"\"\n",
    "class AbstractCustomModel:\n",
    "    \n",
    "    \"\"\"\n",
    "    This is the default constructor for this class.\n",
    " \n",
    "    Parameters:\n",
    "    params : dict - hyperparameters for the model\n",
    "    \"\"\"\n",
    "    def __init__(self,params):\n",
    "        self.params = params\n",
    " \n",
    "    \"\"\"\n",
    "    This method trains the model on the given dataset\n",
    " \n",
    "    Parameters:\n",
    "    x_train : numpy.ndarray - training set data\n",
    "    \"\"\"\n",
    "    def train(self,x_train,y_train=None):\n",
    "        # TODO\n",
    "        pass\n",
    " \n",
    "    \"\"\"\n",
    "    This method evaluates the performance of the model on the given test set.\n",
    " \n",
    "    Parameters:\n",
    "    x_test : numpy.ndarray - test set data\n",
    " \n",
    "    Returns:\n",
    "    A floating point number which measures the performance of the model on the \n",
    "    test set.\n",
    "    \"\"\"\n",
    "    def evaluate(self,x_test,y_test=None):\n",
    "        # TODO\n",
    "        pass\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class is an adapter for the Apyori library.\n",
    "\"\"\"\n",
    "class APyoriAdapter(AbstractCustomModel):\n",
    " \n",
    "    \"\"\"\n",
    "    This is the default constructor for this class.\n",
    " \n",
    "    Parameters:\n",
    "    params : dict - hyperparameters for the model\n",
    "    \"\"\"\n",
    "    def __init__(self,params={}):\n",
    "        self.params = params\n",
    "        super().__init__(params)\n",
    " \n",
    "\n",
    "    def convert_to_transaction(self,row):\n",
    "        return [(i,j)for i,j in enumerate(row)]\n",
    "\n",
    "    \"\"\"\n",
    "    This method trains the model on the given dataset\n",
    " \n",
    "    Parameters:\n",
    "    x_train : numpy.ndarray - training set data\n",
    "    \"\"\"\n",
    "    def train(self,x_train,y_train=None):\n",
    "        x_train = self.discretize_dataset(x_train,self.params[\"thresholds\"])\n",
    "        data_2 = [[i for i,j in enumerate(row) if j==1 ] for row in x_train]\n",
    "        #if running takes long we raise min support \n",
    "        self.results = list(apriori(data_2,min_support=0.199))\n",
    "    \n",
    " \n",
    "    \"\"\"\n",
    "    This method evaluates the performance of the model on the given test set.\n",
    " \n",
    "    Parameters:\n",
    "    x_test : numpy.ndarray - test set data\n",
    " \n",
    "    Returns:\n",
    "    A floating point number which measures the performance of the model on the \n",
    "    test set.\n",
    "    \"\"\"\n",
    "    def evaluate(self,x_test,y_test=None):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    discretize_dataset function transforms the dataset into binary o\n",
    "\n",
    "    Takes the input of:\n",
    "    -data = \n",
    "    -thresholds = \n",
    "    \"\"\"\n",
    "    def discretize_dataset(self,data,thresholds):\n",
    "        temp = [row[::]for row in data]\n",
    "        data = temp\n",
    "        for i, datapoint in enumerate(data):\n",
    "            for j, threshold in enumerate(thresholds):\n",
    "                if datapoint[j] >= threshold:\n",
    "                    datapoint[j] = 1\n",
    "                else: \n",
    "                    datapoint[j] = 0\n",
    "        return data            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule ['IsMajor'] -> IsCalm has\n",
      "            support = 0.23\n",
      "            confidence = 0.65\n",
      "            lift = 2.08\n",
      "\n",
      "            \n",
      "1\n",
      "Rule ['Theme_numbered'] -> IsCalm has\n",
      "            support = 0.21\n",
      "            confidence = 1.49\n",
      "            lift = 4.77\n",
      "\n",
      "            \n",
      "2\n"
     ]
    }
   ],
   "source": [
    "model = APyoriAdapter({\n",
    "    np.random.seed(4815162342)\n",
    "    \"thresholds\" : np.random.normal(0.5, 0.15, num_features)\n",
    "})\n",
    "\n",
    "emotion_val = [1,2,3,4,5,6]\n",
    "emotions = [\"IsCalm\", \"IsCheerful\", \"IsBravery\", \"IsFearful\", \"IsLove\", \"IsSadness\"]\n",
    "lift_supp = [0.3117,  0.1372, 0.1397,  0.2469, 0.0673, 0.0973]\n",
    "model.train(x_train)\n",
    "targets = [list(range(70,75))]\n",
    "\n",
    "#Counters\n",
    "ctr = 0\n",
    "rules = 1\n",
    "confcount = 0\n",
    "list_features = []\n",
    "for ruleset in model.results:\n",
    "    ctr += 1\n",
    "#     print(\"Got {} itemsets\".format(ctr))\n",
    "    for ord in ruleset.ordered_statistics:\n",
    "        if len(ord.items_base) > 0 and len(ord.items_add) > 0:\n",
    "            featurelist = []\n",
    "\n",
    "            for i in ord.items_base:\n",
    "                featurelist.append(features[i])\n",
    "    \n",
    "    list_features = list(featurelist) \n",
    "    \n",
    "    #list of indexes of the feature\n",
    "    colno = []\n",
    "    \n",
    "    #this loops gets the indexes of the columns of the features\n",
    "    for i in list_features:\n",
    "        #this is to find which column sya\n",
    "        colno.append(features.index(i))\n",
    "        \n",
    "    \n",
    "        \n",
    "    #gets the column of the emotion\n",
    "    emotioncol = 69\n",
    "    \n",
    "     #This loops through all emotions\n",
    "    for k in emotion_val:\n",
    "        #This loops through the whole 401\n",
    "        confcount = 0\n",
    "        for i in range(row_count):\n",
    "            #This checks if all is true pa \n",
    "            s = True\n",
    "            for j in colno:\n",
    "                if(x_train[i][j] == 1 and x_train[i][emotioncol] == k and s == True):\n",
    "                    s = True\n",
    "                else:\n",
    "                    s = False  \n",
    "                    \n",
    "            #if all are true pa for that row meaning all the featurelist are 1 for that row, increment \n",
    "            if(s == True):\n",
    "                confcount += 1\n",
    "            \n",
    "            \n",
    "        \n",
    "#         print(confcount)\n",
    "        confidence = (confcount/row_count) / ruleset.support\n",
    "        lift = confidence / lift_supp[emotion_val.index(k)]\n",
    "\n",
    "        if(lift >= 1 and confidence >= 0.5):\n",
    "            print(\"\"\"Rule {} -> {} has\n",
    "            support = {:.2f}\n",
    "            confidence = {:.2f}\n",
    "            lift = {:.2f}\n",
    "\n",
    "            \"\"\".format(list(featurelist),emotions[k-1],ruleset.support,\\\n",
    "                        confidence,lift))\n",
    "\n",
    "            print(rules)\n",
    "            rules += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AbstractCustomModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5f5b3f4a55e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mThis\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0man\u001b[0m \u001b[0madapter\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mKMeans\u001b[0m \u001b[0mapproach\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mKMeansAdapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAbstractCustomModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \"\"\"\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AbstractCustomModel' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This class is an adapter for the KMeans approach.\n",
    "\"\"\"\n",
    "class KMeansAdapter(AbstractCustomModel):\n",
    " \n",
    "    \"\"\"\n",
    "    This is the default constructor for this class.\n",
    " \n",
    "    Parameters:\n",
    "    params : dict - hyperparameters for the model\n",
    "    \"\"\"\n",
    "    def __init__(self,params):\n",
    "        self.params = params\n",
    "        super().__init__()\n",
    " \n",
    "    \"\"\"\n",
    "    This method trains the model on the given dataset\n",
    " \n",
    "    Parameters:\n",
    "    x_train : numpy.ndarray - training set data\n",
    "    \"\"\"\n",
    "    def train(self,x_train,y_train=None):\n",
    "        # TODO\n",
    "        pass\n",
    " \n",
    "    \"\"\"\n",
    "    This method evaluates the performance of the model on the given test set.\n",
    " \n",
    "    Parameters:\n",
    "    x_test : numpy.ndarray - test set data\n",
    " \n",
    "    Returns:\n",
    "    A floating point number which measures the performance of the model on the \n",
    "    test set.\n",
    "    \"\"\"\n",
    "    def evaluate(self,x_test,y_test=None):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
