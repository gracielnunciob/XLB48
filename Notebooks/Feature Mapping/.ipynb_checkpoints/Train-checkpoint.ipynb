{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from csv import reader\n",
    "from csv import writer\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import uniform, norm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, VarianceThreshold, chi2, f_classif, mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from XLB import *\n",
    "from apyori import apriori\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True False False False False False False False\n",
      "  True False False False False False  True False False False  True False\n",
      " False False  True False False False False False  True False False False\n",
      "  True False False False False False  True False False False  True False\n",
      " False False  True False False False False False False  True False  True\n",
      "  True  True  True False False False  True  True  True False  True  True\n",
      " False False  True  True  True False  True False False  True  True False\n",
      " False  True  True False False False  True  True  True False  True  True\n",
      " False False  True  True  True False  True False False False False False\n",
      " False False  True False  True False  True False  True False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False  True False  True  True\n",
      " False]\n",
      "(750, 69) (750,)\n",
      "(750, 69) (750,)\n"
     ]
    }
   ],
   "source": [
    "# extract data from files\n",
    "x_train, y_train = extract_data(\"FinalTrainingSet.csv\")\n",
    "x_val, y_val = extract_data(\"Validation Set.csv\")\n",
    "\n",
    "# scale data values\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "# feature selection\n",
    "num_features = 69\n",
    "feat_sel = VarianceThreshold()\n",
    "x_train = feat_sel.fit_transform(x_train)\n",
    "# 28.57% 0.25\n",
    "# feat_sel_2 = SelectFromModel(estimator=DecisionTreeClassifier(random_state=481516234))\n",
    "# 38.10% 0.29\n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=RandomForestClassifier(n_estimators=100,\\\n",
    "#                                              random_state=481516234))\n",
    "# 42.86% 0.36\n",
    "#feat_sel_2 = SelectFromModel(\\\n",
    "                #estimator=LogisticRegression(random_state=481516234))\n",
    "# \n",
    "# feat_sel_2 = SelectFromModel(\\\n",
    "#                 estimator=svm.LinearSVC(C=0.25, penalty=\"l1\", dual=False,\\\n",
    "#                                     random_state=481516234))\n",
    "# \n",
    "feat_sel_2 = SelectKBest(chi2,k=num_features)\n",
    "x_train = feat_sel_2.fit_transform(x_train,y_train)\n",
    "print(feat_sel_2.get_support())\n",
    "x_val = feat_sel_2.transform(feat_sel.transform(x_val))\n",
    "# print(\"After Variance Threshold Feature Selection:\",x_train.shape)\n",
    "\n",
    "rand_seed = 3454132\n",
    "\n",
    "oversampler = SMOTE(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "x_smote, y_smote = oversampler.fit_resample(x_train,y_train)\n",
    "print(x_smote.shape,y_smote.shape)\n",
    "\n",
    "oversampler = RandomOverSampler(sampling_strategy=\"not majority\",random_state=rand_seed)\n",
    "x_os, y_os = oversampler.fit_resample(x_train,y_train)\n",
    "print(x_os.shape,y_os.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#MOVING FEATURE HEADERS INTO A LIST\n",
    "import csv\n",
    "\n",
    "\n",
    "IsCalm = []\n",
    "IsCheerful = []\n",
    "IsBravery = []\n",
    "IsFearful = []\n",
    "IsLove = []\n",
    "IsSad = []\n",
    "Theme_Number = []  \n",
    "\n",
    "\n",
    "f = open(\"FinalTrainingSet.csv\")\n",
    "reader = csv.reader(f)\n",
    "features = next(reader)\n",
    "\n",
    "\n",
    "csv_temp = pd.read_csv(\"FinalTrainingSet.csv\")\n",
    "Theme_numbered = csv_temp['Theme(Numbered)'].tolist()\n",
    "\n",
    "for i in Theme_numbered:\n",
    "    if i == 1.0:\n",
    "        IsCalm.append(1)\n",
    "        IsCheerful.append(0)\n",
    "        IsBravery.append(0)\n",
    "        IsFearful.append(0)\n",
    "        IsLove.append(0)\n",
    "        IsSad.append(0)\n",
    "    elif i == 2.0:\n",
    "        IsCalm.append(0)\n",
    "        IsCheerful.append(1)\n",
    "        IsBravery.append(0)\n",
    "        IsFearful.append(0)\n",
    "        IsLove.append(0)\n",
    "        IsSad.append(0)\n",
    "    elif i == 3.0:\n",
    "        IsCalm.append(0)\n",
    "        IsCheerful.append(0)\n",
    "        IsBravery.append(1)\n",
    "        IsFearful.append(0)\n",
    "        IsLove.append(0)\n",
    "        IsSad.append(0)\n",
    "    elif i == 4.0:\n",
    "        IsCalm.append(0)\n",
    "        IsCheerful.append(0)\n",
    "        IsBravery.append(0)\n",
    "        IsFearful.append(1)\n",
    "        IsLove.append(0)\n",
    "        IsSad.append(0)\n",
    "    elif i == 5.0:\n",
    "        IsCalm.append(0)\n",
    "        IsCheerful.append(0)\n",
    "        IsBravery.append(0)\n",
    "        IsFearful.append(0)\n",
    "        IsLove.append(1)\n",
    "        IsSad.append(0)\n",
    "    elif i == 6.0:\n",
    "        IsCalm.append(0)\n",
    "        IsCheerful.append(0)\n",
    "        IsBravery.append(0)\n",
    "        IsFearful.append(0)\n",
    "        IsLove.append(0)\n",
    "        IsSad.append(1)\n",
    "        \n",
    "print(IsCalm)\n",
    "# print(IsCheerful)\n",
    "# print(IsBravery)\n",
    "# print(IsFearful)\n",
    "# print(IsLove)\n",
    "# print(IsSad)\n",
    "        \n",
    "\n",
    "#Deleting everything except features from the dataset\n",
    "features.remove(\"Row Labels\")\n",
    "features.remove(\"Theme\")\n",
    "features.remove(\"Theme(Numbered)\")\n",
    "# print(len(features))\n",
    "#Retained features after selection\n",
    "selected_feats = feat_sel_2.get_support(True)\n",
    "\n",
    "for ind, ft in sorted(enumerate(features), reverse=True): \n",
    "    if ind not in selected_feats:\n",
    "        del features[ind]  \n",
    "        \n",
    "        \n",
    "features.append('IsCalm')\n",
    "features.append('IsCheerful')\n",
    "features.append('IsBravery')\n",
    "features.append('IsFearful')\n",
    "features.append('IsLove')\n",
    "features.append('IsSad')\n",
    "\n",
    "# print(features)\n",
    "\n",
    "new_column = pd.DataFrame({'IsCalm': IsCalm})\n",
    "csv_temp = csv_temp.merge(new_column, left_index = True, right_index = True)\n",
    "new_column = pd.DataFrame({'IsCheerful': IsCheerful}) \n",
    "csv_temp = csv_temp.merge(new_column, left_index = True, right_index = True)\n",
    "new_column = pd.DataFrame({'IsBravery': IsBravery}) \n",
    "csv_temp = csv_temp.merge(new_column, left_index = True, right_index = True)\n",
    "new_column = pd.DataFrame({'IsFearful': IsFearful}) \n",
    "csv_temp = csv_temp.merge(new_column, left_index = True, right_index = True)\n",
    "new_column = pd.DataFrame({'IsLove': IsLove}) \n",
    "csv_temp = csv_temp.merge(new_column, left_index = True, right_index = True)\n",
    "new_column = pd.DataFrame({'IsSad': IsSad}) \n",
    "csv_temp = csv_temp.merge(new_column, left_index = True, right_index = True)\n",
    "\n",
    "# column = csv_temp.IsCalm\n",
    "# column1 = csv_temp.IsCheerful\n",
    "# column2 = csv_temp.IsBravery\n",
    "# column3 = csv_temp.IsFearful\n",
    "# column4 = csv_temp.IsLove\n",
    "# column5 = csv_temp.IsSad \n",
    "\n",
    "\n",
    "# print(column)\n",
    "# print(column1)\n",
    "# print(column2)\n",
    "# print(column3)\n",
    "# print(column4)\n",
    "# print(column5)\n",
    "\n",
    "# csv_temp.to_csv('asd.csv', index = False)\n",
    "    \n",
    "f.close()\n",
    "    \n",
    "\n",
    "  \n",
    "#Printing modified list \n",
    "# print (features) \n",
    "# print (len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This abstract class represents a custom machine learning model.\n",
    "\"\"\"\n",
    "class AbstractCustomModel:\n",
    "    \n",
    "    \"\"\"\n",
    "    This is the default constructor for this class.\n",
    " \n",
    "    Parameters:\n",
    "    params : dict - hyperparameters for the model\n",
    "    \"\"\"\n",
    "    def __init__(self,params):\n",
    "        self.params = params\n",
    " \n",
    "    \"\"\"\n",
    "    This method trains the model on the given dataset\n",
    " \n",
    "    Parameters:\n",
    "    x_train : numpy.ndarray - training set data\n",
    "    \"\"\"\n",
    "    def train(self,x_train,y_train=None):\n",
    "        # TODO\n",
    "        pass\n",
    " \n",
    "    \"\"\"\n",
    "    This method evaluates the performance of the model on the given test set.\n",
    " \n",
    "    Parameters:\n",
    "    x_test : numpy.ndarray - test set data\n",
    " \n",
    "    Returns:\n",
    "    A floating point number which measures the performance of the model on the \n",
    "    test set.\n",
    "    \"\"\"\n",
    "    def evaluate(self,x_test,y_test=None):\n",
    "        # TODO\n",
    "        pass\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class is an adapter for the Apyori library.\n",
    "\"\"\"\n",
    "class APyoriAdapter(AbstractCustomModel):\n",
    " \n",
    "    \"\"\"\n",
    "    This is the default constructor for this class.\n",
    " \n",
    "    Parameters:\n",
    "    params : dict - hyperparameters for the model\n",
    "    \"\"\"\n",
    "    def __init__(self,params={}):\n",
    "        self.params = params\n",
    "        super().__init__(params)\n",
    " \n",
    "\n",
    "    def convert_to_transaction(self,row):\n",
    "        return [(i,j)for i,j in enumerate(row)]\n",
    "\n",
    "    \"\"\"\n",
    "    This method trains the model on the given dataset\n",
    " \n",
    "    Parameters:\n",
    "    x_train : numpy.ndarray - training set data\n",
    "    \"\"\"\n",
    "    def train(self,x_train,y_train=None):\n",
    "        x_train = self.discretize_dataset(x_train,self.params[\"thresholds\"])\n",
    "        data_2 = [[i for i,j in enumerate(row) if j==1 ] for row in x_train]\n",
    "        #if running takes long we raise min support \n",
    "        self.results = list(apriori(data_2,min_support = 0.0275))\n",
    "    \n",
    " \n",
    "    \"\"\"\n",
    "    This method evaluates the performance of the model on the given test set.\n",
    " \n",
    "    Parameters:\n",
    "    x_test : numpy.ndarray - test set data\n",
    " \n",
    "    Returns:\n",
    "    A floating point number which measures the performance of the model on the \n",
    "    test set.\n",
    "    \"\"\"\n",
    "    def evaluate(self,x_test,y_test=None):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    discretize_dataset function transforms the dataset into binary o\n",
    "\n",
    "    Takes the input of:\n",
    "    -data = \n",
    "    -thresholds = \n",
    "    \"\"\"\n",
    "    def discretize_dataset(self,data,thresholds):\n",
    "        temp = [row[::]for row in data]\n",
    "        data = temp\n",
    "        for i, datapoint in enumerate(data):\n",
    "            for j, threshold in enumerate(thresholds):\n",
    "                if datapoint[j] >= threshold:\n",
    "                    datapoint[j] = 1\n",
    "                else: \n",
    "                    datapoint[j] = 0\n",
    "        return data            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-6e6c2adaf87c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m })\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m69\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-e507c188ca7f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x_train, y_train)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m#if running takes long we raise min support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapriori\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_support\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.025\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Graciel\\Applications\\Anaconda\\lib\\site-packages\\apyori.py\u001b[0m in \u001b[0;36mapriori\u001b[1;34m(transactions, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msupport_record\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msupport_records\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m         ordered_statistics = list(\n\u001b[0m\u001b[0;32m    288\u001b[0m             _filter_ordered_statistics(\n",
      "\u001b[1;32mD:\\Graciel\\Applications\\Anaconda\\lib\\site-packages\\apyori.py\u001b[0m in \u001b[0;36mfilter_ordered_statistics\u001b[1;34m(ordered_statistics, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mordered_statistic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mordered_statistics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mordered_statistic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfidence\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmin_confidence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Graciel\\Applications\\Anaconda\\lib\\site-packages\\apyori.py\u001b[0m in \u001b[0;36mgen_ordered_statistics\u001b[1;34m(transaction_manager, record)\u001b[0m\n\u001b[0;32m    217\u001b[0m                 record.support / transaction_manager.calc_support(items_base))\n\u001b[1;32m--> 218\u001b[1;33m             \u001b[0mlift\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfidence\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtransaction_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalc_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems_add\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m             yield OrderedStatistic(\n",
      "\u001b[1;32mD:\\Graciel\\Applications\\Anaconda\\lib\\site-packages\\apyori.py\u001b[0m in \u001b[0;36mcalc_support\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mindexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__transaction_index_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mindexes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = APyoriAdapter({\n",
    "    \"thresholds\" : np.random.normal(0.5, 0.15, num_features)\n",
    "})\n",
    "model.train(x_train)\n",
    "targets = [list(range(69,75))]\n",
    "for ruleset in model.results:\n",
    "    valid = False\n",
    "    for target in targets:\n",
    "        for item in ruleset.items:\n",
    "            if target == item:\n",
    "                valid = True\n",
    "                break\n",
    "        if valid:\n",
    "            break\n",
    "    if not valid:\n",
    "        continue\n",
    "    for ord in ruleset.ordered_statistics:\n",
    "        if len(ord.items_base) > 0 and len(ord.items_add) > 0:\n",
    "            featurelist = []\n",
    "            for i in ord.items_base:\n",
    "                featurelist.append(features[i])\n",
    "            \n",
    "            featurelist_2 = []\n",
    "            for i in ord.items_add:\n",
    "                featurelist_2.append(features[i]) \n",
    "                \n",
    "            print(\"\"\"Rule {} -> {} has\n",
    "    support = {:.2f}\n",
    "    confidence = {:.2f}\n",
    "    lift = {:.2f}\n",
    "\n",
    "    \"\"\".format(list(featurelist),list(featurelist_2),ruleset.support,\\\n",
    "                ord.confidence,ord.lift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class is an adapter for the Bayesian Network library.\n",
    "\"\"\"\n",
    "class BayesianNetworkAdapter(AbstractCustomModel):\n",
    " \n",
    "    \"\"\"\n",
    "    This is the default constructor for this class.\n",
    " \n",
    "    Parameters:\n",
    "    params : dict - hyperparameters for the model\n",
    "    \"\"\"\n",
    "    def __init__(self,params):\n",
    "        self.params = params\n",
    "        super().__init__()\n",
    " \n",
    "    \"\"\"\n",
    "    This method trains the model on the given dataset\n",
    " \n",
    "    Parameters:\n",
    "    x_train : numpy.ndarray - training set data\n",
    "    \"\"\"\n",
    "    def train(self,x_train,y_train=None):\n",
    "        # TODO\n",
    "        pass\n",
    " \n",
    "    \"\"\"\n",
    "    This method evaluates the performance of the model on the given test set.\n",
    " \n",
    "    Parameters:\n",
    "    x_test : numpy.ndarray - test set data\n",
    " \n",
    "    Returns:\n",
    "    A floating point number which measures the performance of the model on the \n",
    "    test set.\n",
    "    \"\"\"\n",
    "    def evaluate(self,x_test,y_test=None):\n",
    "        # TODO\n",
    "        pass\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class is an adapter for the KMeans approach.\n",
    "\"\"\"\n",
    "class KMeansAdapter(AbstractCustomModel):\n",
    " \n",
    "    \"\"\"\n",
    "    This is the default constructor for this class.\n",
    " \n",
    "    Parameters:\n",
    "    params : dict - hyperparameters for the model\n",
    "    \"\"\"\n",
    "    def __init__(self,params):\n",
    "        self.params = params\n",
    "        super().__init__()\n",
    " \n",
    "    \"\"\"\n",
    "    This method trains the model on the given dataset\n",
    " \n",
    "    Parameters:\n",
    "    x_train : numpy.ndarray - training set data\n",
    "    \"\"\"\n",
    "    def train(self,x_train,y_train=None):\n",
    "        # TODO\n",
    "        pass\n",
    " \n",
    "    \"\"\"\n",
    "    This method evaluates the performance of the model on the given test set.\n",
    " \n",
    "    Parameters:\n",
    "    x_test : numpy.ndarray - test set data\n",
    " \n",
    "    Returns:\n",
    "    A floating point number which measures the performance of the model on the \n",
    "    test set.\n",
    "    \"\"\"\n",
    "    def evaluate(self,x_test,y_test=None):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
